<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 码上遇见你</title>
    <link>http://vipbbo.com/posts/</link>
    <description>Recent content in Posts on 码上遇见你</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 13 Jan 2023 23:05:22 +0800</lastBuildDate><atom:link href="http://vipbbo.com/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>🔥对线面试官 - HashMap</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-hashmap/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-hashmap/</guid>
      <description>面试官：HashMap了解吗？能简单说说它的底层实现吗？
派大星：有过了解，首先对于HashMap在1.7之前是采用的数组+链表，并且根节点是一个entry节点，它的一个内部类。数据插入过程采用的是一个头插法。头插法会在扩容的过程中调用resize方法，然后又调用transfer方法，把里面的一些entry进行了一些rehash，这个过程可能会造成链表的一个循环死循环，也就是死链。最后一点就是由于它没有加锁，也会导致在多线程并发的情况下是线程不安全的。
派大星：HashMap在1.8之后就进行了升级优化，变成了一个又链表+ 数组+红黑树的一个数据结构，并且把原来的entry节点换成了Node节点，头插法优化成了尾插法。虽然它的插入方法有所改变但是插入顺序没有发生改变，所以不会出现1.7的一个死链。
面试官：不错，上面看你有提到死链的问题，能简单说说1.7为什么会有死链的产生吗？
派大星：好的。由于1.7之前的数组+链表的数据结构和头插法的原因导致了在并发情况下可能会出现死链的情况。具体的表现形式为：当HashMap需要扩容的时候会将旧的HashMap的节点依次转移到新的HashMap中，假设旧的HashMap的链表是A、B、C，而新的HashMap由于采用的是头插法所以最终新的HashMap里面的链表顺序为C、B、A 。如图所示： 在这样的一个背景下，当在多线程并发的情况下，第一步假设线程T1和T2要对HashMap进行一个扩容操作看，此时T1和T2指向的都是链表的头节点A，并且T1和T2的下一个节点(T1.next和T2.next)指向的都是B节点。当线程T2时间片用完进入到休眠状态，而线程T1开始执行扩容操作一直到T1扩容完成后，线程T2才会被唤醒。这个时候最大的问题是线程T2的指向还没有变，因为是头插法，所以新的HashMap的顺序已经改变了。但对线程T2来说它是不知道的。所以它的指向元素没有发生改变。如图： 这个时候线程T2如果恢复，死循环就开始了。因为T1执行完扩容之后B节点的下一个节点是A，而线程T2的指向的首节点是A，下一个节点是B。这个顺序与新的HashMap节点顺序正好相反。T1执行完之后的顺序是B到A，而T2的顺序是A到B，这样A节点和B节点就形成了死循环 面试官：针对HashMap的死链问题有什么样的解决方案嘛？
派大星：可以升级为JDK1.8，或者通过以下方式解决。
使用线程安全容器ConcurrentHashMap替代（推荐使用此方案）。 使用线程安全容器Hashtable替代（性能低，不建议使用）。 使用synchronized或Lock加锁HashMap之后，再进行操作，相当于多线程排队执行（比较麻烦，也不建议使用）。 面试官：JDK1.7到1.8HashMap有什么其它的变化嘛？或者说JDK1.8的HashMap做了哪些性能优化？
派大星：Hash算法的优化概括来说
就是主要表现在对每个Hash值，在它的低16位中，让高低16位进行了异或运算，让它的低16位融合了高低16位的特征。从而尽量避免一些可能会出现的hash冲突，会导致元素进入数组的同一个位置 寻址算法的优化：使用与运算代替了取模，提升性能。 同时为了提升链表的优化性能增加了红黑树的数据结构，来提高HashMap的综合性能。 简单来说就是HashMap在put、get的时候进行了hash算法的优化，避免了hash冲突，同时寻址算法也进行了优化。
面试官：你刚刚提到JDK1.8的HashMap增加了一个红黑树的数据结构。为什么采用红黑树，而不是其它的数据结构呢？
派大星：以典型的AVL树为例，AVL树是一种高度平衡的二叉树，所以查找效率非常高，但是有就有弊，AVL树为了维持这种高度的平衡，就要付出很大的代价，就是每次插入、删除都要做调整，比较复杂且耗时，所以综合考虑虽然红黑树读取略逊于AVL树，但是维护强于AVL，空间开销与AVL类似，内容极多时略优于AVL，维护优于AVL，所以红黑树是有着良好的稳定性和完整的功能综合实力强，在诸如STL的场景中需要稳定的表现。
面试官：不错不错。回答的很好。
派大星：嗯呢，具体的关于HashMap的其它底层实现原理，比如HashMap如何扩容及以上问题的一些细节都可参考之前的文章： 文章地址：</description>
    </item>
    
    <item>
      <title>🔥对线面试官 - TCP经典面试题</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-tcp%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-tcp%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98/</guid>
      <description>面试官：TCP三次握手和四次挥手的工作流程是什么？
派大星：首先说一下TCP三次握手。
第一次握手，客户端发送链接请求报文，此时SYN=1、ACK=0、seq=x，这就是个连接请求此时客户端处于SYN_SENT状态，等待服务器响应。 第二次握手，服务端收到SYN=1的请求报文后需要返回一个确认的报文，ack=x+1，SYN=1，ACK=1，seq=y，发送给客户端，自己处于一个SYN_RECV的状态。 第三次握手，客户端接着又给服务端发送了ack=y+1，ACK=1，seq=x+1 简单总结：其实说白了三次握手就是来回来去的三次请求，每次请求携带上次一堆的TCP报文头，根据报文头是否正确从而建立连接。
面试官：为什么不是五次握手或者两次握手？
派大星： 假设如果是两次握手的话，第一次客户端握手过去结果卡在某个地方了，没有到达服务端。可是客户端再次重新又发送了第一次握手过去，服务端收到了并握手返回，接着彼此就建立了连接。意外的是，之前卡住的第一次握手又死灰复燃发送到了服务端。服务端直接返回一个第二次握手。这个时候服务器也就开辟了一个资源等待接收客户端的数据。可是客户端直接就忽略了该回合的第二次握手，因为之前已经通信过了。如果要是三次握手的话，那个第二次握手发回去之后客户端发现第一次握手已经被丢弃了，就会发送个复位的报文过去，避免了资源的开销。
说白了就是两次握手可能会导致服务端资源的一个浪费。三次握手会有一个复位的报文从而避免这种情况。
派大星：既然三次握手都可以保证连接，四次五次握手就有些浪费资源了。
面试官：不错，继续聊一聊为什么是四次挥手》
派大星：好的。
第一次挥手，客户端发送报文，FIN=1，seq=u，此时进入FIN-WAIT-1状态。 第二次挥手，服务端就收到报文，这是便进入CLOSE_WAIT状态，返回一个报文，ACK=1，ack=u+1 seq=v。客户端收到这个报文后，直接进入到FIN-WAIT-2状态，此时客户端到服务端的连接断开了。 第三次挥手，服务端发送连接释放的报文，FIN=1，ack=u+1，seq=w服务端进入LAST-ACK状态。 第四次挥手，客户端收到连接释放的报文后，发应答报文，ACK=1，ack=w+1，seq=u+1，进入到TIME_WAIT状态，等待一会客户端进入到CLOSED状态，服务端收到报文之后就进入到CLOSED状态。 可参考的文章：https://zhuanlan.zhihu.com/p/125067441</description>
    </item>
    
    <item>
      <title>🔥对线面试官 - 网络经典面试题</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%BD%91%E7%BB%9C%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%BD%91%E7%BB%9C%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98/</guid>
      <description>面试官：如果不在一个子网应该如何传播？
派大星：可以通过路由器/网关。网关就是路由器的一种，运作在网络层。通过交换机走以太网协议进行一个传输数据包。如图：
派大星：交换机主要用来在一个子网/局域网内进行广播。路由器的作用是把不同的子网连接起来。疑问点来了：不同子网的设备通过路由器/网关只知道同一子网的IP地址，不知道对应IP地址的MAC地址啊； 其实这里还有重要的一点是每台机器都有自己的ARP cache，这个ARP就是用来在一个局域网内让各个设备都知道自己每个设备的IP地址和Mac地址的对应关系。这样一广播大家都知道了。
面试官：用浏览器请求一个链接的时候，经历了哪些过程(DNS解析过程) 面试官心里分析：
主要考察的是网络基本功，看看有没有掌握基本的网络通信知识。这个问题相对比较经典。
派大星：假设我们现在有如下配置：
IP地址：192.168.10.110 子网掩码：255.255.255.0 网关地址：192.168.10.1 DNS地址：8.8.8.8
派大星：当我们在浏览器中数据www.baidu.com并回车的时候：1如果在本地的 hosts 文件没有能够找到对应的 ip 地址；会通过DNS服务器将www.baidu.com解析一个IP地址(172.168.10.110)并返回，返回之后本机电脑会判断返回的IP地址与当前IP地址是不是一个子网的(通过将两个IP地址的二进制与子网掩码的二进制进行与运算，并判断前面的3部分二进制是否一样)，得出的结果指定不在一个子网下。
因为两个IP地址不在一个子网内，所以此时只能将数据包先通过以太网协议广播到网关上去，通过网关再发送出去。
2浏览器会请求这个地址，先是按照应用层的http协议封装一个应用层的数据包，数据包里放了http请求报文。3接着会走到传输层，假设这层设置的是TCP协议，此时会把应用层的数据包封装到TCP数据包中去，并携带TCP头，TCP头中包含收发端口号信息。4接下来会走到网络层，这层是IP协议，此时会把TCP头和TCP数据包放到IP数据包里面去，并携带IP头(包含本机192.168.10.110和目标机器的IP地址172.168.10.110)。5下一步走到数据链路层(以太网协议)，并将数据打包成以太网数据包，并携带以太网标头(包含发送者网卡Mac地址、接收者网卡Mac地址-&amp;gt; 对应的是网关192.168.10.1的Mac地址)
注意：以太网的数据包是有大小限制的，最大只能是1500字节。如果一个数据包过大则会进行切分并标记好顺序。 最后会将切割好的以太网数据包发送到网关上去(192.168.10.1)，网关可能会发送到别的网关上去，经过N次这样的转发会转发到目标服务器上
目标服务器接收到切割好的以太网数据包后，会根据IP头的序号将切割分裂的数据包进行拼接并还原出一个完整的数据包，接着一层一层的进行提取知道提取到http请求报文，http请求会直接到应用的Tomcat上去，然后就是Spring MVC，MyBatis这样一个常规的处理流程。
比较好的文章:https://zhuanlan.zhihu.com/p/415022755</description>
    </item>
    
    <item>
      <title>🔥对线面试官-线程入门第一课</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%BA%BF%E7%A8%8B%E5%85%A5%E9%97%A8%E7%AC%AC%E4%B8%80%E8%AF%BE/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%BA%BF%E7%A8%8B%E5%85%A5%E9%97%A8%E7%AC%AC%E4%B8%80%E8%AF%BE/</guid>
      <description>面试官：能聊一聊线程吗？什么是线程？它和进程有什么区别吗？
派大星：假设一个程序例如：weChat.exe这是一个程序，一个静态的概念，双击运行。而进程是相对于程序来说是一个动态的概念。作为进程里面最小的执行单元便是线程。通俗的讲一个程序里不同的执行路径就是线程
面试官：run方法和start方法有什么区别？
派大星：run()方法是一个方法调用。start()方法是Thread类里的一个方法。start()方法会产生一个分支和主方法一同执行。
面试官：线程的几种状态可以简单说一说吗？
派大星：好的
New（新创建） Runnable（可运行）里面有ready和running两个状态 Blocked（被阻塞）未获得锁的时候 Waiting（等待）调用join()、wait()、part()方法 Timed Waiting（计时等待）时间结束的时候。比如：sleep(1000) Terminated（被终止） 面试官：创建线程的几种方式？有了解吗？可以详细聊一聊吗？
派大星：好的。主要有以下几种方式：
继承Thread类，重写run方法。 实现Runnable接口，重写run方法。 当然，还有类似的变种方法：JDK8的lambda表达式的写法。当然还可以通过线程池来启动Executors.newCachedThread当然这种底层也是采用上述两种方式实现的。
面试官：了解线程里的sleep()、yield()、join()``interrupt方法吗？
派大星：sleep()方法会让出CPU让其它线程执行，yield()会让线程重新回到等待队列中让出CPU但是也有可能下次执行的还是当前线程，也就是让线程回到就绪状态。join()方法：假设t1线程执行到中途调用了t2.join()这是t1需要等待t2执行完成之后才会继续执行下去，经常用来等待另外一个线程的结束。
这里就引出了一个经典的线程题：如何保证线程t1、t2、t3依次顺序执行完成。 很简单：只需要在主线程中填加t1.join() t2.join() t3.join() 。或者在t1线程中填加t2.join()，在t2线程中填加t3.join()即可实现。
interrupt方法线程调用interrupt方法打断线程，然后通过方法interrupted方法，可以获取到打断信号。通过这个信号，可以在代码逻辑中停止线程。 通过开关的方式打断异常时，需要定义一个volatile标识的变量，通过判断这个变量来打断线程
面试官：sleep、yield、wait、notify、notifyAll是否释放锁？
派大星：
sleep 和 yield 方法都是不会释放锁的 调用 sleep() 方法使线程进入等待状态，等待休眠时间达到，而调用我们的 yield() 方法，线程会进入就绪状态，也就是sleep()需要等待设置的时间后才会进行就绪状态，而yield会立即进入就绪状态
wait、notify和notifyAll都会释放锁 wait 方法执行后会立即释放锁，等待被唤醒的时候会重新持有锁 notify和notifyAll也会释放锁，但是不是立即释放锁，执行完notify/notifyAll方法后会立即通知其它正在等待的线程，但不是立即释放锁，而是会等到其synchronized内中的代码全部执行完之后，才会释放锁。所以我们一般都时在我们synchronized内的最后才会调用 notify/notifyAll
面试官：wait 与 notify 为什么是 Object 的成员方法？
派大星：
我们都知道，synchronized 关键字可以加在任何对象的成员函数上，任何对象也都可以成为锁。那么，wait() 和 notify() 这样普及，Object 又是所有类的基类，那么 wait() 和 notify() 放在Object 里面最合适不过。wait和nofity不是常见的普通java方法或同步工具，在Java中它们更多的是实现两个线程之间的通信机制。如果不能通过类似synchronized这样的Java关键字来实现这种机制，那么Object类中就是定义它们最好的地方，以此来使任何Java对象都可以拥有实现线程通信机制的能力。 记住synchronized和wait,notify是两个不同的问题域，并且不要混淆它们的相似或相关性。 同步类似竞态条件，是提供线程间互斥和确保Java类的线程安全性的，而wait和notify是两个线程之间的通信机制。 另一个原因：每个对象都可以作为锁 在Java中，为了进入临界区代码段，线程需要获得锁并且它们等待锁可用，它们不知道哪些线程持有锁而它们只知道锁是由某个线程保持，它们应该等待锁而不是知道哪个线程在同步块内并要求它们释放锁。 这个比喻适合等待和通知在object类而不是Java中的线程。</description>
    </item>
    
    <item>
      <title>Helloworld</title>
      <link>http://vipbbo.com/2023/helloworld/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/helloworld/</guid>
      <description>This is My First Blog adsfasdfs
Map </description>
    </item>
    
    <item>
      <title>Java-Map</title>
      <link>http://vipbbo.com/2023/javamap/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/javamap/</guid>
      <description>Java Map解锁新姿势，相信绝大数人并不知晓；相信我，你懂的。
简单Map的操作 说到Java Map的操作，其实有很多种，但是归结起来无非就两种，其中一种便是简单Map的操作，
简单Map 简单Map指的是：value是个atomic value，比如：
Map&amp;lt;K, Integer&amp;gt;; Map&amp;lt;K, Character&amp;gt;; 上面的K是个generic type(也就是泛型)。
复杂的Map 所谓的复杂的Map指的就是，value不是一个atomic value，常见的一个情况就是value是一个collection，比如：
Map&amp;lt;K, Collection&amp;lt;V&amp;gt;&amp;gt; Map&amp;lt;K, List&amp;lt;V&amp;gt;&amp;gt; Map&amp;lt;K, Set&amp;lt;V&amp;gt;&amp;gt; 其实简单的Map操作起来还是比较方便的，这里不再一一举例。 但是通常情况下我们使用的Map更多情况下是一个复杂Map，如果像下面这样编写的一个代码，它就有可能抛出一个NullPointerException
Map&amp;lt;K, List&amp;lt;V&amp;gt;&amp;gt; map = new HashMap&amp;lt;&amp;gt;(); map.get(key).add(val); 上述代码之所以会出现空指针的异常，主要原因就是当map并不包含key时。（map.get(key).add(val)）也就等价于null.add(val) 在我们日常工作开发中通常采用的解决方案，代码如下：
Map&amp;lt;K, List&amp;lt;V&amp;gt;&amp;gt; map = new HashMap&amp;lt;&amp;gt;(); if (!map.containsKey(key)) { map.put(key, new ArrayList&amp;lt;&amp;gt;()); } map.get(key).add(val); 上述方案便可规避掉NullPointerException异常，但是代码看上去过于冗长。
重点来了！解锁复杂Map避免空指针异常的简化代码 Map&amp;lt;K, List&amp;lt;V&amp;gt;&amp;gt; map = new HashMap&amp;lt;&amp;gt;(); map.computerIfAbsent(key, k-&amp;gt; new ArrayList&amp;lt;&amp;gt;()); map.get(key).add(val); 代码是不是看上去简化清爽了许多。 关于computeIfAbsent的Java文档可以参考：computeIfAbsent
当然如此清爽代码的解决方案不止一种，比如还有Guava Multimap
使用Guava Multimap解锁新姿势(该方式代码更加简洁) Multimap&amp;lt;K, V&amp;gt; multimap = ArrayListMultimap.</description>
    </item>
    
    <item>
      <title>Spring Boot自动装配原理</title>
      <link>http://vipbbo.com/2023/spring-boot%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%E5%8E%9F%E7%90%86/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/spring-boot%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%E5%8E%9F%E7%90%86/</guid>
      <description>Spring Boot自动装配原理 </description>
    </item>
    
    <item>
      <title>关于Spring中事务未生效的场景</title>
      <link>http://vipbbo.com/2023/spring-transaction-is-not-valid/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/spring-transaction-is-not-valid/</guid>
      <description>关于Spring中事务未生效的场景 对于Java开发的同学，相信对于Spring的事务在熟悉不过了。但是相信各位小伙伴们在工作中一定有类似于这样的需要： 需要同时写入多张表的数据，为了保证操作的原子性(也就是所谓的要么同时成功，要么同时失败)，避免产生数据不一致的情况，我们都会使用Spring事务，但是对于Spring事务相信大家也遇到过比较诡异的问题，会出现那种事务失效，或者就是不满足自己的业务场景，其实归根结底还是我们针对于Spring事务的某些特殊场景掌握的不够牢靠。接下来我总结了一下关于Spring事务在某些情况下失效的场景，不出意外的话相信你也遇到过。
Spring事务失效的12种场景总结图 Spring 事务不生效 1.访问权限问题 所谓的访问权限问题也就是开发中再熟悉不过的private，default，protected，public，它们的访问权限从左到右，依次变大。如果我们在开发过程中国呢，把某些事务方法定义了错误的访问权限，就会导致事务功能出现问题，甚至失效。例如：
@Service public class UserService { @Transactionsl private void add(User user){ saveUser(user); updateUser(user); } } 上面代码中我们可以看到对于方法add的访问修饰符被定义成了private，这样会导致事务失效，原因是Spring 要求被代理的方法必须是 **public** 的。简单粗暴来看源码是怎么搞的。如下：
/** * Same signature as {@link #getTransactionAttribute}, but doesn&amp;#39;t cache the result. * {@link #getTransactionAttribute} is effectively a caching decorator for this method. * &amp;lt;p&amp;gt;As of 4.1.8, this method can be overridden. * @since 4.1.8 * @see #getTransactionAttribute */ @Nullable protected TransactionAttribute computeTransactionAttribute(Method method, @Nullable Class&amp;lt;?</description>
    </item>
    
    <item>
      <title>基于自定义数据源的LangChain的聊天</title>
      <link>http://vipbbo.com/2023/%E5%9F%BA%E4%BA%8E%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84langchain%E7%9A%84%E8%81%8A%E5%A4%A9/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%9F%BA%E4%BA%8E%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84langchain%E7%9A%84%E8%81%8A%E5%A4%A9/</guid>
      <description>准备好node，pnpm环境。
初始化项目 先创建一个空的文件夹langchain-demo，执行命令以下命令初始化一个项目。
cd langchain-demo/ pnpm init 接下来，我们准备一个测试数据，然后将数据摄取到向量数据库中。具体步骤如下： 首先在根目录下创建一个 ingest-data.js 然后在项目根目录下执行如下命令：
# 安装LangChain pnpm i langchain 安装完成后：修改package.json文件，如图所示，增加&amp;quot;type&amp;quot;: &amp;quot;module&amp;quot;。 读取数据 准备数据 先准备一个markdown格式的文件。可以直接在网上找一个，比如Vue3的官方文档。然后在页面文档选中一部分，打开F12通过输入命令$0.innerHTML并回车后即可获得数据。简单如图所示： 或者自行准备数据也可。 将网页中爬取的文本内容copy到 html-to-markdown网站上，转换成markdown。并将转换后的文件放在项目根目录下的vue3-document.md中，如图所示 读取数据 编辑我们在之前已经准备好的文件ingest-data.js，文件内容如下：
// 读取markdown文件 import { UnstructuredLoader } from &amp;#34;langchain/document_loaders/fs/unstructured&amp;#34;; // 实例化 并传入数据的路径 const unstructuredLoader = new UnstructuredLoader(&amp;#34;./vue3-document.md&amp;#34;); // 读取文件，这是个promise 使用await const docs = await unstructuredLoader.load() console.log(docs) 然后通过如下命令进行验证：
cd langchain-demo/ node ingest-data.js 执行成功结果如图所示： 切割数据 继续编写ingest-data.js具体如下：
// 1. 读取markdown文件 import { UnstructuredLoader } from &amp;#34;langchain/document_loaders/fs/unstructured&amp;#34;; // 2. 文档拆分，将文档拆分成一块一块的 // 2.</description>
    </item>
    
    <item>
      <title>对线面试官 - Java IO经典面试问题突击篇</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-java-io%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E7%AA%81%E5%87%BB%E7%AF%87/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-java-io%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E7%AA%81%E5%87%BB%E7%AF%87/</guid>
      <description>面试官：关于BIO、NIO、以及AIO有了解吗可以简单聊一聊吗？
面试官心理分析：
主要是想考察io这些基础知识的掌握程度，各种流的使用，
派大星：当然可以的。
首先聊一下BIO网络通信原理 最传统的网络通信模型就是BIO，同步阻塞式IO。通俗的讲就是服务端创建一个ServerSocket，客户端用一个Socket去连接那个Server Socket，ServerSocket接收到一个Socket的连接请求就创建一个Socket和一个线程去和那个Socket进行通信。
客户端和服务端的Socket从而进行同步阻塞式的通信，客户端Socket发送一个请求，服务端Socket进行处理后返回响应，响应不许是等处理完后才返回的。
这种方式最大的缺点就是，每一次客户端接入都需要在服务端创建一个线程来服务这个客服端。如果客户端较多的情况下会导致服务端的线程数量激增。从而导致服务器端程序的负载过高，甚至容易崩溃宕机。
优化的点就是可以创建一个线程池，来固定数量处理客户端的请求。但是如果高并发请求的时候也会导致各种的请求排队和延时。毕竟没有那么多的线程去处理。
其次聊一下NIO： JDK1.4中引入了NIO，这是一种同步非阻塞的IO。基于Reactor模型。在NiO中存在一个Buffer的概念，也就是缓冲区，一般都是将数据写入到Buffer中，然后从Buffer中读取数据。具体的Buffer有：IntBuffer、LongBuffer、CharBuffer、等多种针对于基础类型的Buffer。在NIO还有Channel的概念。NIO中都是通过Channel来进行数据读写的；还有selector，这是多路复用器，selector会不断轮询注册的Channel，如果某个Channel上发生了读写事件，selector会将这些Channel获取出来。我们便可通过selector key获取有读写的Channel，就可以进行IO操作。一个selector就通过一个线程便能轮询很多的Channel。这同时也意味着服务端可以接入很多的客户端。
其实具体实现简单的讲就是一个线程处理大量的客户端请求，通过一个线程轮询大量的Channel，每次就获取一批有事件的Channel，然后对每个请求启动一个线程处理即可。实际使用中可以结合CachedThreadPool线程池。
NIO的核心就是非阻塞的。selector一个线程便可以不停地轮询Channel，所有的客户端请求都不会阻塞，直接就会进来，大不了就是等待一下排队而已。
最后说一下AIO： AIO是基于Proactor模型的。就是异步非阻塞模型。 简单来讲：每个连接发送过来的请求都会绑定一个Buffer，然后通知操作系统去异步完成读。此时运行的程序是可以处理别的请求的，等操作系统完成数据读取之后就会调用你的接口，将操作系统异步读完的数据给到你，然后便可以对这个数据进行处理并将结果返回。写的时候也是给操作系统一个Buffer，让操作系统自己获取数据去完成写操作，写完之后通知即可。
与BIO不同的是：BIO的工作线程从Channel中读取数据是同步的，是工作线程自己完成的，同时往Channel中写回数据也是工作线程自己完成。这些动作都是同步的。 而AIO则是异步的。在AIO中工作线程读取数据的时候，只需要提供一个Buffer给操作系统，然后工作线程即可处理别的事情，读数据的动作交给操作系统，操作系统内核会去完成读取数据并将数据放入到提供的Buffer中。操作系统内核完成这一些列读数据的操作后会回调你的接口通知已经完成并将存好数据的Buffer交给工作线程。写数据同理一样的操作。
面试官：不错，掌握的可以，那你可以说说简单聊聊什么同步阻塞、同步非阻塞、异步非阻塞吗？
派大星：可以的，其实上面已经大致说了一些，下面我在简单阐述一下：
为什么BIO叫同步非阻塞呢。其实这个不是针对网络编程模型来说的，是针对文件IO操作来说的，也就是磁盘文件的IO读写(FileInputStream)，因为BIO的流读写文件，指的是发起IO的请求直接hang死，必须等着IO处理完成才能返回。 NIO为什么是同步非阻塞。简单来说就是通过NIO的FileChannel发起的文件IO操作，其实是发起之后就返回了，在这期间可以处理别的事情，这就是非阻塞，只不多接下来你还是需要不断地去轮询操作系统查看IO操作是否完成了。 AIO之所以是异步非阻塞的，简单来说就是当你通过AIO发起文件IO操作之后会立马返回可以处理别的事情，等到操作系统处理完成后会通过回调告诉你已经处理完成了。不需要你主动轮询(这就是同步)。异步是操作系统主动通知你。 面试官：不错。很好。期待你加入我的团队。</description>
    </item>
    
    <item>
      <title>对线面试官 - Java虚拟机对锁的优化</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-java%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AF%B9%E9%94%81%E7%9A%84%E4%BC%98%E5%8C%96/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-java%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AF%B9%E9%94%81%E7%9A%84%E4%BC%98%E5%8C%96/</guid>
      <description>我们之间有聊到过关于Synchronized关键字的相关特性，其主要特性便是保证了原子性和可见性。并且是可重入的。详情可参考文章：
本次给大家带来的便是Java虚拟机对Synchronized后期做了哪些优化模拟一些面试题，希望能够帮助小伙伴。步入正题：
面试官：之前咱们有聊过关于Java关键字Synchronized和Volatile其特性和相关原理。我看你有提到过在JDK1.6对Synchronized关键字做了一些优化，针对这一方面能展开聊聊做了什么优化吗？
派大星：可以的。首先Synchronized在JDK1.6之后引入了锁升级(偏向锁、自旋锁、重量级锁)的概念，而非是直接申请操作系统级别的重量级锁，这本身就是优化的一个点。其次细节上的优化大致有如下几个点：
锁消除：锁消除是JIT编译器对Synchronized锁做的优化之一，其方式就是在编译的时候，JIT会通过逃逸分析技术来分析Synchronized锁对象是不是只可能被一个线程来枷锁，没有其它线程来竞争枷锁，如果是这样的话，编译的时候就不需要monitorenter和monitorexit的指令。也就是说当仅有一个线程争用锁的时候，就可以消除这个锁了。 锁粗化：锁粗化的意思是在JIT编译器编译的时候如果发现代码里存在连续多次加锁释放锁的代码，会将其合并为一个锁，也就是将锁粗化了。避免了频繁多次加锁释放锁。 偏向锁：首先因为Synchronized中的monitorenter和 monitorexit指令是要使用CAS操作加锁和释放锁的，开销相对比较大，因此如果发现大概率只有一个线程会主要竞争一个锁，那么此时就会给这个锁维护一个偏好(Bias)，后面它加锁和释放锁，基于Bias来执行，不需要通过CAS。这样性能也会提升很多。但是如果出现来其它的线程来竞争这个锁，此时就会将分配给之前那个线程的Bias偏好收回。 轻量级锁：由于竞争锁太过激烈，偏向锁没有成功实现，此时就会尝试使用轻量级锁的方式来加锁，其操作就是将对象头的Mark Word里的轻量级指针(monitor)指向自己，然后去和ObjectMonitor(是monitor指针指向外部的一个C++实现对象)里加锁的的线程是不是一个，如果是一个则会重入加锁，如果不是一个那么就会加锁失败。 面试官：不错，那你知道CAS硬件底层原理是怎么实现的吗？
派大星：其实底层是和MESI有关系的，简单来讲就是通过独占锁的机制（这里其它处理器的数据就会全部失效），然后将数据查询出来，并进行比较，最后再写回去。也就是CAS操作。</description>
    </item>
    
    <item>
      <title>对线面试官 - MQ之如何保证消息的顺序性及消息积压问题</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-mq%E4%B9%8B%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E7%9A%84%E9%A1%BA%E5%BA%8F%E6%80%A7%E5%8F%8A%E6%B6%88%E6%81%AF%E7%A7%AF%E5%8E%8B%E9%97%AE%E9%A2%98/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-mq%E4%B9%8B%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E7%9A%84%E9%A1%BA%E5%BA%8F%E6%80%A7%E5%8F%8A%E6%B6%88%E6%81%AF%E7%A7%AF%E5%8E%8B%E9%97%AE%E9%A2%98/</guid>
      <description>书接上文。这次继续聊一聊MQ
面试官：如何保证消息的顺序性，可以简单聊聊什么场景需要避免这种问题的发生以及如何解决吗？
$\color{red} {Red} $ 这里是红色
$\color{ Pink} {我} \color{rgb(0,248,0)} {就}\color{#008000} {喜}\color{Purple} {欢}\color{Indigo} {每}\color{DarkSlateBlue} {个}\color{Blue} {颜}\color{DarkBlue} {色}\color{SlateGray} {都}\color{ CadetBlue} {不}\color{PaleTurquoise} {同}$
派大星 ：当然可以。其实就MySQL而言来说，比如需要依赖MySQL 的binlog做一个同步系统。用户在MySQL中增删改一条数据，对应出来增删改3条binlog，接着将这3条binlog同步到MQ里面。然后到消费出来一次执行，此时就需要保证消息的顺序性，不然就会出现问题。
面试官：嗯，不错那你简单说说不同MQ消息错乱的一个场景吗？
派大星：可以的。
首先来说RabbitMQ：一个queue，多个consumer，这就会出现问题；因为多个消费者是同步一起执行的，无法保证顺序性，并且也无法保证消费者消费到了哪条数据。简单如图所示： **解决方案：**每个消费者建立对应的queue，并且让保持顺序的消息只发送到一个queue上，这样消费者消费数据处理的时候就不会出现顺序错乱。
其次说一下Kafka： 首先来说Kafka是可以保证生产者写入一个partition的数据一定是有顺序的。
在Kafka使用中，只要Kafka内部不涉及多个线程并发处理的情况下，其实我们只需要在生产者写入数据的时候可以指定一个key，比如指定某个订单id作为key，这个订单相关的数据就会被分发到一个partition。这里我们要知道一个原则是Kafka一个partition只能被一个消费者消费，这样消费者从partition中取出来的数据一定是有顺序的。
面试官：什么情况下Kafka会出现消息顺序不一致的情况呢？
派大星：当消费者内部搞多个线程并发处理的时候，则可能会出现顺序不一致的问题。如图所示： 面试官：那如何解决Kafka多线程处理导致的消息错乱问题呢？
派大星：其解决方案可以是采用hash算法进行hash分发。相同的订单key的数据分发到同一个内存queue里面去。如图所示： 面试官：嗯，不错。那你在实际使用过程中有遇到过消息积压的问题吗？能说说遇到这种问题的时候你的解决思路是什么样的？
派大星：好的。其实在面对消息有积压的情况。多数都是消费者故障导致的。简单的解决思路如下： 可以当做是一个临时紧急扩容的一个方案：
如果consumer有问题，先修复consumer的问题，确保其恢复消费速度。然后将现有的consumer都停掉。 临时建立好原先10倍或20倍的queue的数量。(Kafka-新建一个topic，partition是原来的十倍) 写一个临时分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时处理，直接均匀轮询写入已经建立好10/20倍数量的queue中。 接着临时征用10倍的机器来部署consumer，每一批consumer来消费一个临时queue的数据。 最后等快速消费完积压的数据之后，得恢复原来部署架构，重新使用原来的consumer机器来消费。 面试官：不错。思路不错。突然有个问题，如何解决RabbitMQ中消息延时过期失效的问题？
派大星：RabbitMQ有一个TTL过期时间，关掉不要开启TTL。</description>
    </item>
    
    <item>
      <title>对线面试官 - MQ数据丢失问题的解决方案</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-mq%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-mq%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</guid>
      <description>书接上文。这次继续聊一聊MQ
面试官：继上次你对MQ的回答，我还有一些疑问想要针对MQ。我们继续聊一聊？
派大星：好的，当然可以。
面试官：OK，那我们继续上次的话题，就是MQ如何保证消息的可靠性，或者说如何保证消息不丢失呢？
派大星：这种情况需要就不同情况进行分析。主要是有三张场景会导致消息丢失的问题。
生产者丢失了消息 MQ丢失了消息 消费的时候丢失了消息 面试官：嗯，不错，那你能就每种情况简单聊一聊吗?
派大星：可以，首先我先简单说一下RabbitMQ丢失消息如何解决。每种消息丢失的情况的解决方案大致如下图所示： 首先来说一说生产者丢失了消息： 主要场景是：写消息等过程中消息还没有到达MQ的时候，在网络传输的过程中就将消息丢失了；或者消息到了RabbitMQ但是MQ内部错乱没有存储消息导致消息丢失。
解决方案1：可以使用RabbitMQ事务机制，具体配置如下：
channel.txSelect(); try{ // 发送消息 }catch{ channel.txRollback(); } channel.txCommit(); 但是该种方案也有弊端：因为是事务机制，所以是同步阻塞的，这样就会导致生产者发送消息的吞吐量大大下降 解决方案2：把channel设置成confirm模式，发送一个消息就不用管了，RabbitMQ如果接收到了这个消息就会回调生产者本地的一个接口，通知你说这条消息已经发送成功并且接受成功，反之也会通知。这种方式的吞吐量也会高一些。
其次说一些RabbitMQ自己弄丢了消息 这种情况的解决方可可以将RabbitMQ设置为持久化。除非有及其罕见的情况RabbitMQ还没来得及持久化自己就挂了，可能会导致少量的数据丢失，当然这种概率是很小的。
最后便是第三种情况：消费者丢失了消息 只有当你打开了消费者的autoAck的这样一个机制：你消费到了数据之后消费者会自动通知RabbitMQ说我已经消费到了这条数据；这样会出现一种情况：假设你消费到了一条数据但是还没有处理完，此时消费者就自动autoAck了。此时恰巧消费者系统服务挂了，消息还没来得及处理而且RabbitMQ以为该消息已经处理掉了。 解决方案便是关掉RabbitMQ的自动ACK机制
面试官：不错，刚刚你有提到RabbitMQ设置持久化。你知道它怎么配置持久化吗：
派大星：直到的。具体步骤如下(注意两者缺一不可，需同时设置)：
创建queue的时候将其设置为持久化的，这样保证RabbitMQ持久化queue的元数据，但是不会持久化queue里的数据 另外发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化。此时RabbitMQ就会将消息持久化到磁盘上去。 面试官：不错，但是我们这边实际工作中用的MQ是Kafka居多，关于Kafka消息丢失就以上情况你了解具体的解决方案吗？
派大星：这个也了解一些。
首先说一下。Kafka中消费者弄丢了消息的场景： 具体过程为消费者自动提交了offset，其实消息还没有处理完。和RabbitMQ情况差不多。 解决方案为：就是关闭自动提交offset，手动提交offset
其次说一下Kafka弄丢了消息 主要表现形式为：Kafka的leader接受到了消息但是还没来得及同步给follwer就挂了，此时follwer变成了leader。导致数据丢失。 解决方案为：需要设置4个参数：
给topic设置replication.replicas参数，这个值必须要大于1，也就是要求每个parttion只要有两个副本。 在Kafka服务端设置min.insync.replicas参数：这个值必须要大于1，这个是要求一个leader只要高指导最少有一个follwer还跟自己保持联系，这样才能确保leader还有一个follwer。 在producer段设置ack=all：这个要求是每条数据必须是写入所有的replica之后，才能认为是成功了。 在producer端设置retries=MAX：这个要求一旦写入失败，就无线重试，卡在这里。 按照上述配置后至少保证了在Kafka段在leader所在的broker发生故障进行leader切换时，数据不会丢失。
最后聊一下生产者丢失数据的情况 如果是按照上述方式配置了ack=all则一定不会丢，要求是：你的leader接收到消息，所有的follwer都同步到了消息之后，才认为本次消息发送成功，否则生产者会重试无限次。</description>
    </item>
    
    <item>
      <title>对线面试官 - MySQL隔离级别 锁机制</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-mysql%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB-%E9%94%81%E6%9C%BA%E5%88%B6/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-mysql%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB-%E9%94%81%E6%9C%BA%E5%88%B6/</guid>
      <description>面试官：MySQL如何实现的Read Repeatable的？ 派大星：MySQL是通过MVCC机制来实现的，就是多版本并发控制，multi-version concurrency control。 innodb存储引擎，会在每行数据的最后加两个隐藏列，一个保存行的创建事件，一个保存行的删除事件，但是这儿存放的不是时间，而是事务id，事务id是mysql自己维护的自增的，全局唯一。 事务id，在mysql内部是全局唯一递增的，事务id=1，事务id=2，事务id=3 在一个事务内查询的时候，mysql只会查询创建时间的事务id小于等于当前事务id的行，这样可以确保这个行是在当前事务中创建，或者是之前创建的；同时一个行的删除时间的事务id要么没有定义（就是没删除），要么是必当前事务id大（在事务开启之后才被删除）；满足这两个条件的数据都会被查出来。
面试官：那么如果某个事务执行期间，别的事务更新了一条数据呢？
派大星：这个很关键的一个实现，其实就是在innodb中，是插入了一行记录，然后将新插入的记录的创建时间设置为新的事务的id，同时将这条记录之前的那个版本的删除时间设置成刚刚的新的事务的id。现在get到这个点了吧？这样的话，你的这个事务其实对某行记录的查询，始终都是查找的之前的那个快照，因为之前的那个快照的创建时间小于等于自己事务id，然后删除事件的事务id比自己事务id大，所以这个事务运行期间，会一直读取到这条数据的同一个版本。创建事务id &amp;lt;= 当前事务id&amp;lt; 删除事务id Tips：
基于undo log多版本链条以及ReadView机制实现的多事务并发执行的RC隔离级别、RR隔离级别，就是数据库的MVCC多版本并发控制机制。 RR 关键点在于每次查询都生成新的ReadView RC 不会生成新的ReadView 面试官：不错，那我们继续聊一聊MySQL锁类型有哪些吧？
派大星：表锁，行锁，和页锁(几乎很少使用)。
MyIsam使用的就是表锁，在默认情况下执行查询的时候会加个表共享锁，也就是表读锁。这个时候其他请求只能查询数据不能修改数据。MyIsam写的时候也会加个表独占锁也就是表写锁，其它请求不能读也不能写。 行锁有两种：分别是共享锁(s)、和排它锁(x)。InnoDB常用的就是行锁，(当然它也有表锁)。 InnoDB在insert 、update、delete以上操作都会加行级排它锁。select则不会加锁，因为InnoDB默认实现了可重复读，也就是mvcc机制。所以多个事务随便读一个数据，一般不会有什么冲突。
面试官：OK，如何手动添加共享锁、排它锁。简单说说？
派大星：ok。简单代码如下：
共享锁 select * from table where id = 1 lock in share mode; 排它锁(悲观锁) select * from table where id = 1 for update; 面试官：不错，了解悲观锁和乐观锁是什么吗，具体的使用场景是什么？
派大星：
MySQL中的悲观锁指的就是select * from table where id = 1 for update。简单理解就是它担心自己拿不到锁，所以会先锁定，不允许其他请求再获得锁。同时不能加共享锁也不能加排它锁。 乐观锁：相对来说就比较简单。就是它觉得不会有其他请求与其争抢锁。所以它不需要提前获得锁。一般都是通过版本号来确定select id, name, version from table where id =1 具体使用场景如下： 悲观锁：</description>
    </item>
    
    <item>
      <title>对线面试官 - TCP_IP四层网络模型经典连环问</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-tcp_ip%E5%9B%9B%E5%B1%82%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%BB%8F%E5%85%B8%E8%BF%9E%E7%8E%AF%E9%97%AE/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-tcp_ip%E5%9B%9B%E5%B1%82%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%BB%8F%E5%85%B8%E8%BF%9E%E7%8E%AF%E9%97%AE/</guid>
      <description>面试官：TCP、IP四层模型有了解吗？可以简单说说嘛。
派大星：主要包括数据链路层、网络层、传输层、应用层。
面试官：可以简单聊聊什么是OSI七层网络模型吗？
派大星：可以的，其实它和四层网络模型主要区别是多了表示层、会话层、物理层，依次顺序是应用层``表示层、会话层、传输层、网络层、数据链路层、物理层
面试官：可以简单聊一聊各个层不同的含义，存在的意义吗？
派大星：当然：首先来说： 从底向上的4层模型：
物理层：所谓的物理层就是指将各个电脑直接通过某种介质(WiFi、网线)连接起来形成一个网络，这就是物理层的含义。物理层负责传输0和1的电路信号。 数据链路层：架构在物理层之上，定义一些协议，将电路信号0和1进行分组。后来统一固定为以太网协议，一组电信号就是一个数据包又叫一个帧，每帧分成两个部分，标头(head)和数据(data)，标头会包含一些说明性的东西，比如发送者接收者和数据类型等等。 数据链路层：网络交换机就是工作在该层的。网络交换机是通过MAC地址来寻址和传输数据包的；但是路由器/网关是通过IP地址寻址和传输数据包的。网络交换机主要用在局域网的通信。一般你假设一个局域网，里面的电脑通过数据链路层发送数据包，通过MAC地址来广播的，广播的时候就是通过网络交换机这个设备来吧数据广播到局域网内的其它机器上去的。路由器一般用来让你接入英特网。
这里涉及到了以太网协议，它规定了只要接入网络的设备都必须要有一个网卡，以太网规定了，每个网卡必须包含一个Mac地址，Mac地址是这个网卡的唯一标识，唯一的Mac地址是一个48位的二进制，但是一般为了方便使用12个16进制数据表示，前6个事厂商编号，后6个16进制是网卡流水号。 然后通过广播的方式在同一个子网内进行传播。
网络层：上面说通过广播的方式将数据包在同一个子网内传播出去，那么是确定是同一个子网、局域网呢？那这个时候就需要网络层。 在网络层中最重要的就是IP协议，IP协议可以给每个电脑定义一个IP地址(IPV4、IPV6)，范围是0.0.0.0~255.255.255.255，这里的IP地址中前24个二进制位(也就是前3组十进制的数据)中表示的是网络，后8位(最后一组十进制)代表了主机。 运算公式为：通过将两个IP地址的二进制与子网掩码的二进制进行与运算，并判断前面的3部分二进制是否一样，一样则是一个子网。否则不是一个子网/局域网
传输层：这里有个问题是。一台机器上很多程序使用的都是一个网卡进行通信的。这些软件都是从一个网卡往外面发送数据，完后并从该网卡接收数据。如何区分不同应用软件接收的不同数据呢，此时就需要引入一个端口(范围是0~65536、0~1023被系统占用了)的概念。从上面说的我们可以发现网络层是基于IP协议进行主机间的寻址和通信的，传输层则是建立在主机的某个端口上到另外主机的某个端口上完成连接和通信的这个通信是通过socket来实现的。通过socket就可以基于tcp/ip协议完成上述所说的基于IP地址和MAC地址转换和寻址。以及通过路由进行通信然后建立一个端口到另外一个端口的连接。 TCP/UDP都是传输层的协议，作用就是在数据包里面加入端口号，可以通过端口号进行点对点通信。UDP是不可靠的。TCP是可靠的。
应用层：通过TCP协议可以完成端口对端口的一个通信，但是通信成功后拿到的这个数据应该如何去处理，这里就涉及到了应用层。比如最常见的应用层协议就是http协议(按照什么格式封装的就需要按照什么格式去解析响应。)。进行网络通信。 派大星：总结，其实最常见的就是四层网络协议，分别是：数据链路层(以太网协议)、网络层(IP协议)、传输层(TCP协议)、应用层(http协议) 所谓的七层就是引入了物理层(网线、光缆)、会话层、表示层、应用层 这三层一个并也就是四层里的应用层了</description>
    </item>
    
    <item>
      <title>对线面试官 - 各个MQ的经典面试题</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E5%90%84%E4%B8%AAmq%E7%9A%84%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E5%90%84%E4%B8%AAmq%E7%9A%84%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98/</guid>
      <description>书接上文继续聊聊MQ的问题吧。
面试官：继上次聊的MQ的问题，想再问问有了解过MQ如何保证其高可用行吗？这个可以简单聊聊吗
派大星：当然可以。
首先说一说Rabbit MQ的高可用性 Rabbit MQ有三种模式**：单机模式-（demo级别的，生产很少使用，这里就无需再说啦）、**普通集群模式以及镜像集群模式
简单说一下普通集群模式：它是非分布式非高可用。意思就是在多个机器上启动多个Rabbit MQ实例，每个机器启动一个，但是你创建的queue只会放在一个Rabbit MQ实例上，但是每个实例都去同步queue的元数据。这样在你消费的时候实际上如果连接到了另外一个实例，那么这个实例会去queue所在的实例上将数据拉取过来。这种方式很麻烦并且没有做到所谓的分布式，就是普通的集群。这样会导致要么消费者每次随机连接一个实例然后拉取数据，要么就固定连接那个queue的实例消费数据，前者有数据拉取的开销，后者导致单实例的瓶颈。 它的优点是这种模式只是提高了消费者消费的吞吐量。缺点也显而易见：其一就是可能会在Rabbit MQ集群内部产生大量的数据传输，再者就是可用性没有什么保障，如果queue所在的节点宕机了，数据就丢失了，因为那个queue所在的实例包含元数据和实际数据。
接着说一下镜像集群模式简单如图所示： 这种模式才是所谓的Rabbit MQ真正的高可用模式，与普通集群模式不同的是：你创建的queue无论是元数据还是queue的消息会存在于多个实例上，每次写消息到queue的时候，都会自动把消息与多个实例的queue进行消息同步。 它的优点就是其中某个机器宕机了，别的机器还可以继续提供服务。缺点：这个性能相比较而言开销较大，消息需要同步所有消息。导致网络带宽压力和消耗很重。还有一点就是所谓的扩展性几乎没有，因为假设某个queue的数据负载很重，加机器无法线性去扩展queue。
面试官：嗯，不错。那你知道如何开启Rabbit MQ的镜像模式吗？
派大星：其实就是在管理控制台新增一个镜像集群的策略，要求所有节点同步数据。
面试官：嗯，可以。那你知道Kafka的高可用性如何保证吗？
派大星：首先我们要有个基本的认识，简单如图所示： 我们都知道Kafka是多个broker组成，每个broker是一个节点，你创建一个topic，这个topic可以划分为多个partition，每个partition可以存在不同的broker上，每个partition就放一部分数据。天然的分布式消息队列。因为一个topic数据是分散在多个机器上的。每个机器之存放一部分数据。
Tip：
Kafka0.8之前是没有HA（高可用）机制的。就是任何一个broker宕机了，那么这个broker上的partition就废了，没法写也没有办法读。 Kafka0.8以后，提供了HA机制，就是replica副本机制，每个partition的数据都会同步到其它机器上，形成自己的多个replica副本。然后所有replica会选举出一个leader出来，那么生产和消费都和这个leader打交道，然后其它的replica就是follower。写的时候leader只负责把数据同步到follwer上，读的时候直接读leader。如果leader宕机follwer会变成leader，follwer变成leader过程无法提供对外服务。
面试官：很好，那在生产环境中如何保证消息不被重复消费呢？或者说如何保证消息的幂等性。
派大星：首先针对于Kafka来说，出现这种情况的原因是：消费者offset没来得及提交导致重复消费。 大致可参考下图： 面试官：那你能说说针对幂等性问题有什么解决方案吗？
派大星：方案需要根据不同的场景做不同的应对。 情况一：如何生产者不重复发送消息到MQ。可以通过让mq内部可以为每条消息生成一个全局唯一、与业务无关的消息id，当mq接收到消息时，会先根据该id判断消息是否重复发送，mq再决定是否接收该消息。 情况二：如何保证消费者不重复消费。其重点在于如何让消费者保证不重复消费的关键在于消费者端做控制，因为MQ不能保证不重复发送消息，所以应该在消费者端控制：即使MQ重复发送了消息，消费者拿到了消息之后，要判断是否已经消费过，如果已经消费，直接丢弃。所以根据实际业务情况，有下面几种方式：
如果从MQ拿到数据是要存到数据库，那么可以根据数据创建唯一约束，这样的话，同样的数据从MQ发送过来之后，当插入数据库的时候，会报违反唯一约束，不会插入成功的。（或者可以先查一次，是否在数据库中已经保存了，如果能查到，那就直接丢弃就好了）。 让生产者发送消息时，每条消息加一个全局的唯一id，然后消费时，将该id保存到redis里面。消费时先去redis里面查一下有么有，没有再消费。（其实原理跟第一点差不多）。 如果拿到的数据是直接放到redis的set中的话，那就不用考虑了，因为set集合就是自动有去重的。 面试官：不错。可不可以再聊聊不同MQ如何保证消息传输的可靠性呢？或者说如何处理消息丢失的问题。
派大星：额&amp;hellip; .要不下次说吧。这次有点累啦。</description>
    </item>
    
    <item>
      <title>对线面试官 - 硬件级别之再谈Volatile关键字的可见性</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%A1%AC%E4%BB%B6%E7%BA%A7%E5%88%AB%E4%B9%8B%E5%86%8D%E8%B0%88volatile%E5%85%B3%E9%94%AE%E5%AD%97%E7%9A%84%E5%8F%AF%E8%A7%81%E6%80%A7/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%A1%AC%E4%BB%B6%E7%BA%A7%E5%88%AB%E4%B9%8B%E5%86%8D%E8%B0%88volatile%E5%85%B3%E9%94%AE%E5%AD%97%E7%9A%84%E5%8F%AF%E8%A7%81%E6%80%A7/</guid>
      <description>继之前讲多的Synchronized和**volatile**关键字，本篇文章会再深入从硬件级别带你去了解其特性。
之前文章也有提到过：
Synchronized既保证了原子性也保证了可见性、可重入（自己不停地加锁） volatile主要是保证线程可见性，禁止指令重排序
文章参考：
面试官：你知道为什么volatile无法保证原子性。只可能保证可见性和有序性？
派大星：针对于Volatile关键字对原子性的保障在Java里是很有限的，我觉得几乎可以忽略不计。比如在32位的Java虚拟机里面，对long、double变量的赋值写不是原子性的，此时可以通过给变量加Volatile关键字来保证在32位Java虚拟机里面对long、double的赋值写是原子性的。相反int i = 0 原子性是Java语言规范(比如甲骨文)就规定了。
面试官：不错，那么为什么long、double在32位Java虚拟机里面的简单赋值操作不是原子性的。
派大星：所有变量的简单赋值操作，Java语言规范都给你保证原子性的。但是例外的是long和double在32位虚拟机里面的简单赋值操作不是原子性的。因为long和double是64位的。 如果多线程情况下同时并发执行long = 30 ，由于long是64位的，就会导致有的线程在修改l的高32位，有的线程在修改long的低32位，多线程并发给long类型的变量进行赋值操作，在32位虚拟机是有问题的。产生的结果导致 a 的值不是30，可能是-3333334430，也就是乱码一样的数字，因为高低32位赋值错了，就导致二进制转换为十进制之后是一个很奇怪的数字。
面试官：可以从硬件级别的谈一下可见性问题吗？或者说硬件级别为什么会有可见性问题？
派大星：好的。简单可从下面几种情况分析
每个处理器都有自己的一个寄存器(register)，所以多个处理器各自运行一个线程的时候，会将主内存中的某个变量副本给加载到寄存器里面，然后对其进行更新。这样就会导致各个线程没法看到其它处理器里的变量值修改了。所以就引发了可见性的第一个问题：寄存器级别的变量副本的更新，无法让其它处理器看到 还有一个问题是，处理器运行的线程对变量的操作针对的都是写缓存(store buffer)，并不是直接更新主内存，所以很可能导致一个线程更新了变量，但是仅仅只是在写缓存中进行了更新，并没有直接更新到主内存中去。这个时候其他线程也就无法读取到它的写缓冲区里面的变量值的。所以也导致了可见性的问题 每一个处理器都有自己的高速缓存，处理器运行的线程对变量的操作可能更新到写缓冲器里面，也可能更新到高速缓存中去或者是主内存中。但是其它处理器还是从自己的高速缓存或者写缓冲器中读取的变量值，此时还是读取的旧值，非新值。 面试官：既然硬件级别是有可见性问题的，那么是如何解决的呢？
派大星：硬件级别要想实现可见性，其中一个方法就是通过MESI协议。这个MESI协议在之前的文章也有提过但是并没有展开说。根据具体底层硬件的不同 ，MESI协议的实现也是有些区别的。
面试官：可以简单说说MESI的实现方式吗？
派大星：可以的，MESI协议实现如下： 就是一个处理器将另外一个处理器的高速缓存中的更新后的数据拿到自己的高速缓存中来更新一下，这样彼此之间的缓存就实现了同步，然后各个处理器之间的线程看到的变量数据也就是一样的了。当然为了实现MESI协议，其中是有两个配套的专业机制的。flush 处理器缓存、refresh处理器缓存
首先说一下flush处理器缓存其目的是把自己更新的变量值刷新到高速缓存里(或主内存中去)，并且它还会发送一个消息到总线(bus)，通知其它处理器某个变量值被它更新了。这样才可让其它的处理器从自己的高速缓存(主内存)里读取到更新的值。 其次就是refresh处理器缓存，它的目的是处理器中的线程在读取一个变量的时候，如果发现总线(bus)嗅探中有消息说其他处理器的线程更新了该变量的值，则必须从其它处理器的高速缓存(或主内存)中读取到这个最新的值，并更新到自己的高速缓存中去。 总的来说，为了保证可见性，在底层是通过MESI协议、flush处理器缓存和refresh处理器缓存，这一套机制来保障的。并且其实内存屏障的使用，在底层硬件级别的原理，其实就是在执行flush和refresh</description>
    </item>
    
    <item>
      <title>对线面试官 - 硬件级别之再谈Volatile关键字的指令重排序</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%A1%AC%E4%BB%B6%E7%BA%A7%E5%88%AB%E4%B9%8B%E5%86%8D%E8%B0%88volatile%E5%85%B3%E9%94%AE%E5%AD%97%E7%9A%84%E6%8C%87%E4%BB%A4%E9%87%8D%E6%8E%92%E5%BA%8F/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%A1%AC%E4%BB%B6%E7%BA%A7%E5%88%AB%E4%B9%8B%E5%86%8D%E8%B0%88volatile%E5%85%B3%E9%94%AE%E5%AD%97%E7%9A%84%E6%8C%87%E4%BB%A4%E9%87%8D%E6%8E%92%E5%BA%8F/</guid>
      <description>面试官：之前从硬件级别聊了可见性的相关问题。这次能能简单从硬件级别聊聊指令重排吗？
派大星：当然可以，说到有序性就需要提到编译后的代码的执行顺序：Java中有两种编译器，一个是静态编译器(javac)，一个是动态编译器(JIT)。javac负责把.java文件中的源代码编译为.class文件中的字节码，这个一般是程序写好之后进行编译的。而JIT则是负责把.class文件中的字节码编译为JVM所在操作系统支持的机器码。一般在程序运行过程中进行编译。 为什么要提到这两个编译器呢？原因就是由于在这个编译过程中，编译器是很有可能调整代码的执行顺序，从而提高代码的执行效率。尤其是JIT编译器对指令重排序还是挺多的。同时处理器也有可能对JIT编译好的指令进行重排序执行。还有一种情况则是：内存重排序，简单来说就是处理器在实际执行指令的过程中。在高速缓存和写缓冲器或者无效队列等等硬件层面的组件，也可能会导致指令的执行顺序是有变化的。导致其它处理器看到你的执行顺序是错乱的。 但是话又说回来，指令重排序不是胡乱的排序，它们会遵循一个关键的规则，就是数据依赖规则，如果说一个变量的结果依赖于之前的代码执行的结果，那么就不能随意进行重排序，要遵循数据的依赖。此外，之前有提到过的happens-before原则，就是有一些基本的规则时要遵守的，不会让你胡乱重排序
面试官：new 一个新的对象，JIT会存在指令重排序的可能吗？(JIT指令重排序的经典案例)
派大星：存在指令重排序的可能。具体情况如下： 假设我们编写这样一行代码：MyObject myObj = new MyObject();这行代码大致会执行三个步骤。
步骤一：以MyObject类作为原型，给它的对象实例分配一块空间。 如下objRef就是指向了分配好的内存空间的地址的引用。 objRef = allocate(MyObject.class); 步骤二：针对分配好内存空间的对象实例执行它的构造函数，并对这个对象实例进行初始化操作。主要就是执行我们自己写的构造函数里的一些代码，对各个实例变量赋值，初始化的逻辑。 invokeConstructor(objRef); 步骤三：步骤一步骤二执行完成后，一个对象实例就创建完成了。此时就是把objRef指针指向的内存地址，赋值给我们自己的引用类型的变量。此时myObj就可以作为一个类似指针的概念指向了MyObject对象实例的内存地址。属于是给我们自己创建的myObj变量进行赋值操作。 myObj = objRef; 以上就是new对象的一个过程，但是JIT动态编译为了加快程序的执行速度，步骤2是在初始化一个对象实例(比如里面执行一些磁盘读写，网络通信等等)，JIT动态编译则可能重排为步骤一 、步骤三、步骤二。 但是一旦这样进行指令重排则有可能会出现一些问题，可看如下代码：
public class MyObject{ private Resource resource; public MyObject(){ this.resource = loadResource();// 从配置文件里加载数据构造Resource对象 } public void execute(){ this.resource.execute(); } } // 线程1执行 MyObject myObj = new MyObject() // 线程2执行 myObj.execute(); 一旦出现线程1 刚刚执行完步骤一和步骤三，步骤二还没有执行，此时myObj已经不是null，但是MyObject对象实例内部的resource还是null。 线程2直接执行了myObj.execute()方法，此时内部会调用resource.execute()方法，但是此时resource还是空的就会报NPE。 其实DCL单例模式中就是会出现JIT指令重排，如果不加Volatile关键字则会有一些问题的发生。Volatile是保证了步骤1、2、3全部执行完毕后，才可使用myObj对象。
面试官：不错，理解的很透彻呀。
面试官：那你了解处理器的指令乱序执行的机制吗？
派大星：有过一些了解简单来说就是：
指令乱序执行机制：指令不一定拿到了就可以立马执行，比如有的执行需要网络通信、磁盘读写、获取锁等等，有的指令不是立马就执行的，为了提升效率，在现代的很多处理器里面走的都是乱序执行的机制。乱序执行完成后，会将每个指令的执行结果放到重排序处理器里面，重排序处理器的作用是把各个指令的结果按照接收指令时的顺序放到硬件组件里面去(高速缓存或写缓冲器里面)。 猜测执行机制：所谓的猜测执行机制就是很有可能会执行if里的代码算出来结果后，再去判断if条件是否成立。这样也会导致代码的执行顺序和想象的不一样。 面试官：你能聊聊上面你提到过的内存指令重排序吗？(高速缓存和写缓冲器的内存重排序造成的视觉假象)
派大星：可以。首先我们要知道一个概念就是： 处理器将数据写入缓冲器里，这个过程被称之为store，从高速缓存中读取是数据，这个过程是load。写缓冲器和高速缓存执行load和store的过程都是按照处理器指示的顺序执行的。处理器的重排序处理器也是按照程序顺序执行的load和store的。 但是有个问题：就是在其它处理器可能会看到的一个视觉假象，就是有可能出现看到的load和store的执行顺序并非真实的执行顺序而是重排序的。这也就是内存重排序。 内存重排序一共有4种：
LoadLoad重排序：一个处理器先执行一个L1读操作，再执行一个L2读操作；但是另外一个处理器看到的是先L2再L1. StoreStore重排序：一个处理器先执行一个W1写操作，再执行一个W2写操作，但是另外一个处理器看到的是先W2再W1.</description>
    </item>
    
    <item>
      <title>对线面试官 - 绝无仅有真实线上问题排查面试题突击篇</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%BB%9D%E6%97%A0%E4%BB%85%E6%9C%89%E7%9C%9F%E5%AE%9E%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E9%9D%A2%E8%AF%95%E9%A2%98%E7%AA%81%E5%87%BB%E7%AF%87/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%BB%9D%E6%97%A0%E4%BB%85%E6%9C%89%E7%9C%9F%E5%AE%9E%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E9%9D%A2%E8%AF%95%E9%A2%98%E7%AA%81%E5%87%BB%E7%AF%87/</guid>
      <description>Tips：涉及到的命令会在后面统一整理出来方便使用。
面试官：如果线上服务器CPU 100%了，该如何排查、定位和解决？这个有遇到过吗？可以简单聊聊吗？ 面试官心理分析
主要是想要考察你有没有处理过高负载的线上问题场景。
派大星：首先我们要通过定位具体是哪个进程耗费CPU，可以使用top -c命令。这样可以显示进程列表，然后输入P，便会按照CPU的使用率进行排序。如图所示： 通过top -c定位到具体是哪个进城后，接下来要定位耗费CPU的线程；可以通过命令top -Hp pid，这里的pid就是进程id。然后输入P，就会按照cpu的使用率排序。如图所示： 找到耗费CPU较高的线程之后，便可以通过命令printf &amp;quot;%x\n&amp;quot; pid这里命令是要把线程pid换成16进制的。如图所示： 然后通过命令jstack 进程id | grep &#39;0x加上刚刚计算出来的16进制&#39; -C5 -color打印堆栈信息。 比如：jstack 9529 | grep &#39;0x4418&#39; -C5 -color 这样就可已锁定是哪个方法出现了问题。
面试官：如果遇到线上进程kill不掉怎么办？你应该如何排查处理？ 面试官心理分析：
主要是想要考察你有没有遇到过线上的问题，如何应对及处理
派大星：很巧，在线上确实有遇到过类似的问题，在说解决问题的方案的时候先聊一下我们线上发布流程背景。 我们有自己的一套线上发布的流程系统，就是在每次部署的时候会从git仓库上去自动拉去代码，然后根据profile，Maven会自动打对应环境的包。然后进行发布。系统发布是由发布系统的进程创建的子进程去发布的。 当时出现一个现象是无论怎么kill 系统的进程都无法杀死，后来通过命令ps aux查看到系统进程的哪种数据的status那列是zombie状态，也就是僵尸进程。 解决方案可通过ps -ef | 僵尸进程id ，找到父进程，然后kill掉父进程即可。
面试官：嗯，可以。确实有处理过线上问题。回去等消息吧。看好你喔。
线上CPU 100%排查命令： # 获取哪个进程耗费CPU 输入命令后按p排序 top -c # 定位耗费CPU的线程 top -Hp pid # 将进程id转换为16进制 printf &amp;#34;%x\n&amp;#34; pid # 打印堆栈信息即可锁定 jstack 9529 | grep &amp;#39;0x4418&amp;#39; -C5 -color 无法kill进程相关排查命令 # 查找zombie僵尸状态的进程，并锁定父进程 ps aux ps -ef | 僵尸进程id </description>
    </item>
    
    <item>
      <title>对线面试官 Redis _ 十 Redis集群模式</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-redis-_-%E5%8D%81-redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-redis-_-%E5%8D%81-redis%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/</guid>
      <description>面试官：Redis集群模式有了解过吗？具体的工作原理可以简单说一下吗？
面试官的心理分析： 主要考察的是在实际工作中是否接触过海量数据+高并发+高可用的场景。以及Redis这几年的一个变化。因为在前几年Redis本身不支持 集群模式，只能依赖于一些中间件，比如codis、推特的twemproxy等。但是最近几年Redis官方支持了集群模式Cluster，在实际应用中 该集群模式也被广泛使用，面试官主要是想考察了解你对Redis的一个掌握程度。
派大星：有了解，前几年主要通过Redis的一些中间件来做，比如codis、推特的twemproxy等，但是最近几年Redis官方支持了集群模式 Cluster。
面试官：那你简单介绍一下Redis Cluster吧。
派大星：好的，Redis Cluster它会自动将数据进行分片，同时每个master只有一部分数据。提供内置的高可用支持，部分master不可用 时还是可以对外提供服务的。在Redis Cluster架构下，每个Redis需要开发两个端口，比如一个是：6379 一个是16379；另外一个就是 加1w的端口号，比如16379。端口号用来进行节点之间的通信，也被称为cluster bus。cluster bus的通信用来故障检测、配置更新、故 障转移授权。cluster bus用了另外一种二进制协议，gossip 协议，用于节点间进行高效的数据交换，占用较少的网络带宽和处理时间
面试官：不错，可以继续讲一讲节点间的内部通信机制吗
派大星：可以，首先来讲一般集群元数据的维护方式有两种：
集中式、gossip协议，Redis采用的是gossip协议进行通信。 面试官：这两个有什么区别吗？
派大星：当然，首先针对与集中式来说，它是将集群元数据(节点信息，故障信息)等集中存储在某个节点上，这种存储方式有点典型的代 表就是大数据中storm 。它是分布式的大数据实时计算引擎，是集中式的元数据存储的结构，底层基于zookeeper对所有元数据进行存 储维护。如图： 集中式的好处就在于：对于元数据的读取和更新时效性 非常好，一旦出现数据的变更，就立即更新到集中式存储中，其它节点也可以 第一时间进行感知到。缺点就是所有的元数据都在一个地方。可以会导致元数据的存储有压力。
派大星：其次对于goosip 协议来讲它的优点相比较于集中式存储它的元数据信息相对比较分散，不集中在一个地方，更新请求会分散 在其它有元数据的节点，从而降低了压力。相对来说时效性没有那么好。会有一些滞后。
面试官：gossip协议你了解多少，可以简单介绍以下嘛？
派大星：gossip协议包含很多信息，比如ping 、 pong 、 meet 、 fail 等等。
meet：某个节点会发送meet给新加入的节点，然后新节点就会加入集群中，并于其它节点进行通信 ping：每个节点都会给其它节点频繁的发送ping，其中包含自己元数据信息，互相通过ping来交换元数据信息。 pong：返回ping和meet，包含自己的状态和其它信息，用于信息广播和更新。 fail：某个节点判断另外一个节点fail后，就会给其它节点发送fail。通知其它节点告诉某个节点宕机了。 派大星：ping是要携带一些元数据，如果很频繁的话可能会导致网络开销较大。并且每个节点每秒会执行10次的ping，每次会选择5个 最久没有通信的其它节点。当然这期间如果发现了某个节点通信延时达到了(cluster_node_time_out / 2 )。那么就会立即发送平，避免 数据交换延时过长，落后的时间太长了。比如说两个节点已经有10分钟没有交换数据了，那么整个集群就存在严重的数据不一致的情况 ，就会有问题。所以cluster_node_timeout 可以去调节,调节的比较好的好。ping频率自然也就降低了。
面试官：既然Redis Cluster有这么多的节点，它是如何做的选择呢，也就是怎么做的节点负载均衡？
派大星：说到节点负载均衡就需要知道分布式的寻址算法：
hash算法：（大量缓存重建） 一致性hash算法：(自动缓存迁移)+虚拟节点(自动负载均衡) Redis cluster的Hash slot算法 以上三种算法Redis中采用的是Hash slot算法。
面试官：那你能简单介绍下这三种算法吗？
派大星：好的， 首先Hash算法，会根据key计算其Hash值，然后对节点数取模，接着打在不同的master节点上。一旦某一个节点宕机，所有请求打过来，都会基于最新的剩余的master节点进行取模尝试获取数据。这样就会导致后续所有请求过来，都无法拿到有效的缓存，导致大量的请求直接打到DB上。 对于一致性Hash算法而言，它会将整个Hash值空间组装成一个虚拟的圆环，整个空间按顺时针方向流转，下一步将各个master节点进行Hash，这样就能确定每个节点在其Hash环位置上。请求过来的时候，会先根据key计算Hash值并确定此数据在环上的位置，从此位置沿着环的顺时针方向行走，遇到的第一个master节点就是key所在的位置。这种情况即使有一个节点挂了宕机了，受影响的数据仅仅是此节点到环空间前一个节点(也就是沿着逆时针方向行走遇到的第一个节点)之间的数据，其它不会受到影响，增加一个节点也同理。相比较于Hash算法即使挂了一个节点不会使后面的所有请求都直接打到DB上。但是一致性Hash算法如果在节点特别少的情况下，容易出现因为节点分布不均匀而造成缓存热点的问题。为了解决这种热点问题，一致性Hash算法引入了虚拟节点机制，即对每一个节点计算多个Hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。 至于Redis Cluster的Hash slot算法。首先Redis Cluster中有固定的16384个Hash slot，对每个key计算CRC16 值，然后对16384取模，可以获取key对应的Hash Slot。 Redis Cluster中每个master都会持有部分slot，比如有3个master，那么每个master可能会持有5000多个Hash Slot。Hash Slot让node的增加移除变得简化，增加一个master，将其它master的hash slot移动部分过去即可，减少一个slot，将它的hash slot移动到其它的master即可。移动hash slot的成本也很低。针对客户端的API可以针对指定的数据，让其走同一个Hash Slot，通过Hash Tag即可以实现。 这样就保证了任何一台机器宕机，其它的节点都不受影响，因为key要找的是Hash Slot而不是机器。</description>
    </item>
    
    <item>
      <title>对线面试官-Redis 九 _ 持久化的方式及优缺点</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-redis-%E4%B9%9D-_-%E6%8C%81%E4%B9%85%E5%8C%96%E7%9A%84%E6%96%B9%E5%BC%8F%E5%8F%8A%E4%BC%98%E7%BC%BA%E7%82%B9/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-redis-%E4%B9%9D-_-%E6%8C%81%E4%B9%85%E5%8C%96%E7%9A%84%E6%96%B9%E5%BC%8F%E5%8F%8A%E4%BC%98%E7%BC%BA%E7%82%B9/</guid>
      <description>面试官：Redis持久化有了解吗？可以聊一聊吗？
面试官心理分析： redis 如果仅仅只是将数据缓存在内存里面，如果 redis 宕机了再重启，内存里的数据就全部都弄丢了啊。你必须得用 redis 的持久化机制，将数据写入内存的同时，异步的慢慢的将数据写入磁盘文件里，进行持久化。 如果 redis 宕机重启，自动从磁盘上加载之前持久化的一些数据就可以了，也许会丢失少许数据，但是至少不会将所有数据都弄丢。 这个其实一样，针对的都是 redis 的生产环境可能遇到的一些问题，就是 redis 要是挂了再重启，内存里的数据不就全丢了？能不能重启的时候把数据给恢复了？
派大星：可以的，Redis持久化分为两种：
RDB持久化：是对Redis中数据执行周期性的持久化，具有时效性。 AOF持久化：把每条写入命令作为日志，以append-only的模式写入一个日志文件中，在Redis重启的时候，可以通过回放AOF日志的写入命令来重新构建整个数据库。 面试官：那你知道RDB和AOF这两种方式有什么区别吗？或者它们有什么优缺点？ 派大星： 首先来说对于RDB来讲：
优点：首先对于RDB来说：它具有时效性，因为该特性所以它会有多个数据文件，每个数据文件都代表Redis中某一时刻的数据。这种方式非常适合做冷备，可以将这种完整的数据文件发送到远程的安全存储上，比如S3，阿里云的ODPS，从而以一个预定好的策略来定期备份Redis数据。并且该方式对Redis的读写服务影响非常小，从而也能让Redis保持高性能，因为Redis主进程只需要fork一个子进程让子进程执行磁盘IO操作来进行RDB持久化即可。有点类似于Java序列化，恢复速度相对比较快。 缺点：由于其时点性，所以它不支持拉链，永远只有一个dump.rdb文件，同时也会造成**时点与时点之前窗口数据容易丢失(8点得到一个rdb文件 9点刚要得到rdb文件 挂机了)。**RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。 派大星：其次来说AOF，
优点：丢失数据相对较少或不丢失，一般AOF会每隔1秒，通过后台线程执行一次fsync操作，最多丢失1s的数据，AOF日志文件的命令是通过可读性非常好的方式进行记录，所以这个特性非常适合做灾难性的误删除的紧急恢复(比如某人不小心用 flushall 命令清空了所有数据,只要这个时候后台 rewrite 还没有发生)。 缺点：同样的备份文件AOF相比较于RDB，文件会比较大，恢复速度也比较慢，单纯使用AOF的话由于其每秒都会fsync一次日志文件的特性，会导致写的时候的QPS相比较于RDB的QPS要低一些。Redis性能也会有所降低。 派大星：当然在Redis4.0之后也有所更新，具体如图所示： 当然AOF也会产生一写小的问题就是，进行数据恢复的时候，有可能不会恢复出一摸一样的数据。因为基于AOF这种较为复杂的基于命令日志/merage/回放的方式，比基于RDB每次持久化一份完整的数据快照文件的方式更加脆弱，容易有bug。不过AOF就是为了避免rewrite过程导致bug，因此每次rewirte的时候并不是基于旧的指令日志进行merge的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性就会好一些。 面试官：不错，那么在实际生产中，如何选择呢？ 派大星：首先来说不要仅仅使用RDB，因为时点性的原因会导致丢失很多数据，同时也不要仅仅使用AOF，因为那样会有两个问题，第一通过AOF做冷备，没有RDB的体积小恢复的速度快，第二，RDB每次简单粗暴的生成快照文件更加健壮，可以避免AOF这种复杂的备份和恢复机制的bug。Redis目前是支持同时开启这两种方式的持久化的，我们可以综合使用AOF和RDB两种持久化机制，用AOF来保证数据不丢失，作为数据恢复的第一选择；用RDB来做不同程度的冷备，在AOF文件都丢失或损坏不可用的是有可以采用RDB来进行快速的数据恢复。（同时Redis4.0之前Redis中RDB和AOF可以同时开启但是只会用AOF恢复，但是在4.0之后AOF中包含RDB全量增加记录的写操作）。 面试官：非常不错，我对你这边的情况还是比较满意的。 派大星：谢谢。</description>
    </item>
    
    <item>
      <title>对线面试官-Redis(八 基于哨兵HA的原理)</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-redis%E5%85%AB-%E5%9F%BA%E4%BA%8E%E5%93%A8%E5%85%B5ha%E7%9A%84%E5%8E%9F%E7%90%86/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-redis%E5%85%AB-%E5%9F%BA%E4%BA%8E%E5%93%A8%E5%85%B5ha%E7%9A%84%E5%8E%9F%E7%90%86/</guid>
      <description>面试官：之前聊了基于哨兵模式的Redis高可用，那哨兵如何部署才能保证故障转移成功呢？ 派大星：哨兵集群必须部署 2 个以上节点，如果哨兵集群仅仅部署了 2 个哨兵实例，quorum = 1。 如果 master 宕机， s1 和 s2 中只要有 1 个哨兵认为 master 宕机了，就可以进行切换，同时 s1 和 s2 会选举出一个哨兵来执行故障转移。但是同时这个时候，需要 majority，也就是大多数哨兵都是运行的。
2 个哨兵，majority=2 3 个哨兵，majority=2 4 个哨兵，majority=2 5 个哨兵，majority=3
如果此时仅仅是 M1 进程宕机了，哨兵 s1 正常运行，那么故障转移是 OK 的。但是如果是整个 M1 和 S1 运行的机器宕机了，那么哨兵只有 1 个，此时就没有 majority 来允许执行故障转移，虽然另外一台机器上还有一个 R1，但是故障转移不会执行。 经典的 3 节点哨兵集群是这样的： 配置 quorum=2，如果 M1 所在机器宕机了，那么三个哨兵还剩下 2 个，S2 和 S3 可以一致认为 master 宕机了，然后选举出一个来执行故障转移，同时 3 个哨兵的 majority 是 2，所以还剩下的 2 个哨兵运行着，就可以允许执行故障转移。 面试官：哨兵集群的自动发现机制了解吗？ 派大星：这个我知道，哨兵之间的发现其实现方式是通过Redis的pub/sub，每个哨兵都会网__sentinel__:hello这个channel里发送一个消息，这个时候所有其它哨兵都可以消费到这个消息，并感知到其它哨兵的存在。</description>
    </item>
    
    <item>
      <title>对线面试官-线程池(总结篇)</title>
      <link>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%BA%BF%E7%A8%8B%E6%B1%A0%E6%80%BB%E7%BB%93%E7%AF%87/</link>
      <pubDate>Fri, 13 Jan 2023 23:05:22 +0800</pubDate>
      
      <guid>http://vipbbo.com/2023/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%BA%BF%E7%A8%8B%E6%B1%A0%E6%80%BB%E7%BB%93%E7%AF%87/</guid>
      <description>创建线程的方式 主要有两大类方式：
通过Executors创建（6种） 通过ThreadPoolExecutorPools创建（1种） Executors.newFixedThreadPool() 注意：这里主要是考察你实际到底用没用过。真正使用过的一定会说这些创建方式的优缺点。 ！！！不建议使用Executors创建线程：
FixedThreadPool 和 SingleThreadPool允许的请求队列长度为Integer.MAX_VALUE，从而可能会堆积大量请求。造成OOM（因为newFixedThreadPool中队列的实现是LinkedBlockingQueue而LinkedBlockingQueue 的最大容量是 Integer.MAX_VALUE）源码如下： CachedThreadPool和 ScheduledThreadPool 允许的创建线程数量为Integer.MAX_VALUE可能会创建大量的线程，从而导致 OOM。 Executors 突击面试可忽略 每个线程池的具体demo
Executors.newFixedThreadPool：创建一个固定大小的线程池，可控制并发的线程数，超出的线程会在队列中等待 public static void fixedThreadPool() { // 创建 2 个数据级的线程池 ExecutorService threadPool = Executors.newFixedThreadPool(2); // 创建任务 Runnable runnable = new Runnable() { @Override public void run() { System.out.println(&amp;#34;任务被执行,线程:&amp;#34; + Thread.currentThread().getName()); } }; // 线程池执行任务(一次添加 4 个任务) // 执行任务的方法有两种:submit 和 execute threadPool.submit(runnable); // 执行方式 1:submit threadPool.execute(runnable); // 执行方式 2:execute threadPool.execute(runnable); threadPool.execute(runnable); } Lambda表达式的写法：</description>
    </item>
    
    <item>
      <title></title>
      <link>http://vipbbo.com/1/java-spi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://vipbbo.com/1/java-spi/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>http://vipbbo.com/1/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E8%BF%9B%E7%A8%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://vipbbo.com/1/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E8%BF%9B%E7%A8%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1/</guid>
      <description>面试官：能简单聊聊进程间的通信方式吗？
派大星：可以的，主要方式为：管道、命名管道、消息队列、共享内存。
面试官：可以简单介绍一下这些通信方式吗？
派大星：好的。 首先说一下：
管道(pipe)： unix操作系统里面，有一个fork操作，可以创建进程的子进程，或者说是复制一个进程完全一样的子进程，共享代码空间，但是各自有独立的数据空间，不过子进程的数据空间是拷贝父进程的数据空间的。
管道机制要求的是两个进程之间是有血缘关系的。就比如fork出来的父子进程。
Linux操作系统里面，管道用来缓存要在进程间传输的数据，管道是一个固定大小的缓冲区，大小为4kb。管道中的数据一旦被读取出来，就不在管道里面了。但是如果管道满了，那么写管道的操作就阻塞了，直到别人读了管道的数据；反之如果管道是空的，那么读操作的管道就阻塞了。管道一边连着一个进程的输出，一遍连着进程的输入，然后就一个进程写数据另外一个进程读数据、如果两个进程都没了管道也就没了。管道是半双工的，就是数据只能流向一个方向，就比如说你架设一个管道，只能一个进程写，一个进程读。
Linux里面对管道的实现是使用了两个文件，指向了一个VFS(虚拟文件系统)的索引节点node，然后VFS索引节点指向一个物理页面，接着一个进程通过自己关联的那个文件写数据，另外一个进程通过自己关联的那个文件读数据。
其次聊一下：
命名管道(FIFO) 管道的通信要求必须是父子关系的进程间通信，这样就受到了限制。所以可以用命令管道来解决这个问题。
之前的管道是没有名字的，所以必须是有父子关系的进程才可以使用。但是命名管道是有名字的。这个命名管道相当于是有一个名字的文件，是有路径的。所以没有血缘关系的进程都可以通过这个命名管道来进行通信，名字在文件系统上，数据在内存里。其它的和管道一样：一个进程写，一个进程读。也是半双工。数据只能单向滚动
然后聊聊：
消息队列 Linux的消息队列可以认为是一个链表结构。Linux内核有一个msgque链表，这个链表里的每个指针指向一个msgid_ds结构，这个结构就描述了一个消息队列。然后进程间就通过这个消息队列即可完成通信。益阳市写入数据和消费数据，消费队列的好处是对每个消息可以指定类型，消费的时候就消费指定类型的消息即可。功能也相对比较多一些。
最后说一下：
共享内存 一块物理内存被映射到两个进程的进程地址空间，所以进程之间互相都可以立即看到对方在共享内存里做出的修改。有利有弊由于其是共享内存，所以需要锁来保证同步，这里就涉及了Linux底层的一些东西了。不做展开，可自行了解。
面试官：不错，你知道线程见如何切换吗?简单聊聊？
派大星：这个了解一些。 简单的讲就是一个进程的多个线程间切换的时候就涉及到了上下文切换。简单来说就是有一个时间片算法，CPU给每个线程一个时间片来执行，时间片结束之后，就保存当前线程的状态然后切换到下一个线程去执行。这就是所谓多线程并发执行的原理。就是多个线程来来回回的切换。每个线程就一个时间片里执行。底层原理可自行去了解哈。
如有问题，欢迎加微信交流：w714771310，备注- 技术交流 。或关注微信公众号【码上遇见你】。</description>
    </item>
    
    <item>
      <title></title>
      <link>http://vipbbo.com/1/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-redis-%E5%8D%81%E4%B8%80-_-%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://vipbbo.com/1/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-redis-%E5%8D%81%E4%B8%80-_-%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/</guid>
      <description>Redis双写一致性问题解决方案的终结篇
在之前的文章中有介绍过关于缓存一致性的问题，那么为什么还要出一篇文章来再次说明呢？是因为之前的文章主要讲述了高并发架构下缓存一致性问题可以通``延时双删进行解决，高可用架构(读写分离)采用的是先更新数据库，然后再删除缓存，并最后采用重试机制进行避免。之前文章的高可用解决方案后续优化的点侧重于结合binglog的方式去解决。这篇文章又结合了JVM队列的方式。具体有细节可以阅读本篇文章。相信对你在实际应用中会有很大帮助，至于实际应用采用什么方式。还需要结合实际的业务场景。
这片文章更侧重于高可用架构也就是读写分离架构的解决方案是如何实施的。
面试官：在实际的工作中，你们Redis是如何保证缓存与数据库的双写一致性呢？
面试官心里分析：主要考察实际工作中到底是使用没使用过Redis，因为使用过Redis的话一定会遇到双写一致性的问题。那么如何解决的呢？需要简单描述一下当时的方案，这样才能确保你确实是使用过而不是简单的背背八股文，没有实战经验的小白。
派大星：对于这个问题其实在实际应用中，Redis最经典的方式就是Cache Aside Pattern，也就是缓存+数据库读写模式。
读的时候，先读缓存缓存没有的话，就读数据库然后把数据库的数据再次放入到缓存中。同时返回响应。 更新的时候，先更新数据库，然后再删除缓存。 面试官：那为什么是先更新数据库，而不是先更新缓存/删除缓存呢？
派大星：其实这样操作的原因很简单，在实际的业务场景中，缓存应用是有相对比较复杂的情况的，因为缓存有可能不单单是数据库中取出来的值，而是通过查询数据库某些值然后运算后才放入的缓存中，这样来说如果不是热点数据的话，每次更新数据库都去更新缓存，这样其实对于这种比较复杂的缓存数据的情景下更新缓存的代价还是比较高的。 还有另外一点就是为什么删除缓存而不是更新缓存？这里其实就是一个lazy计算的思想。不管它是否用到每次都去重新做复杂的计算，建议采用当其被需要使用的时候再去重新计算。想MyBatis``Hibenate他们都是有懒加载的思想的。
派大星：其次其实为什么是先更新数据库，而不是先更新缓存/删除缓存的原因也很简单。这种方式只能是解决掉简单的缓存架构(高并发架构)的双写一致性的问题(当然这种解决法方式在高并发的情况下也是有线程安全问题，真正的解决方案是延时双删) 。具体思路在之前的文章有提到过:
面试官：ok，那你具体说说双写一致性的问题到底应该如何解决？即具体的实施方案。
派大星：好的，其实采用之前文章的延时双删的方案在流量并不是很多的情况下已经可以解决。但是如果是亿万级流量或者流量真的很高的情况下。采用那种方案是远远不够的。具体如何解决其实可以参考一下之前的文章: 这一次提供一个不一样的解决方案：
派大星：首先更新数据的时候，可以根据数据的唯一标识，将操作路由之后，发送到一个JVM内部队列中。读取数据的时候，如果发现数据不在缓存中，那么此时应该采用重新读区数据+更新缓存的操作，根据唯一标识路由之后，也发送到同一个JVM队列中。 一个队列对应一个工作线程，每个工作线程串行拿到对应的操作之后一条一条的执行。这样操作，一个数据的变更就会按章先删除缓存然后再去更新数据库，然后在更新缓存。即使在更新数据库没完成的时候来了一个请求没有读到缓存，那么也可以将它的更新缓存的操作发送到队列中，此时JVM队列就会有积压，直到同步等待缓存更新完成。
当然这样操作也是有一个优化点的：一个队列中，其实多个更新缓存请求串联在一起是没有意义的，因此我们可以在这里做一些过滤。如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新缓存的请求进去了。只需要等待前面的缓存更新请求完成即可。 等待那个对应的队列的工作线程完成了上一个操作的数据的修改之后，才会执行下一个操作。也就是更新缓存的操作。此时把数据库中最新的值读取出来写入缓存即可。 当然如果请求在等待的时间范围内，我们可以不断的轮询直到取到值返回即可，但是如果等待时间超过了一定时长我们可以直接将数据库中的旧值读取并返回。
面试官：这样设计的话在高并发场景下不会有问题吗？
派大星：是会有一些问题的。比如：
读请求长时间阻塞 读请求长时间阻塞。由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题。每个读请求一定要控制在超时时间范围内返回。其次的问题就是该解决方案，最大的风险点在于：可能数据更新很频繁，导致队列中积压了大量的更新操作在里面，然后读请求会发生大量的超时，最后导致大量的读请求直接请求到数据库上。针对这种情况需要针对真实的业务场景模拟真实的测试环境，来查看具体的更新频率如何。 还有一点就是，因为在一个队列中，可能会积压针对多个数据项的更新操作，因此这就需要根据实际业务场景来进行测试。比如可能需要部署多个服务，每个服务分摊一些数据的更新操作。如果一个队列中积压了100个商品的库存修改操作，每个库存修改操作耗费10ms，那么最后一个商品的去请求，可能会等待10*100=100ms=1s后，才能得到这个数据，这个时候就导致了去请求的长时间阻塞(这里一定要注意模拟实际生产环境到底可能会积压多少个更新请求在队列中，越多hang的时间则就越长)。如果真的是一个内存队列中积压的更新操作特别多，此时就需要考虑加机器了让每个机器部署的服务实力处理更少的请求。这样每个内存队列中积压的更新操作才会更少。
读请求并发量过高。 另外一种风险就是：读请求并发量过高。就是突然之间大量读请求会在短时间内(比如几十毫秒)的延时hang在服务上。看看服务能不能扛得住。需要多少机器才能抗住最大的极限情况的峰值。但是因为并不是所有的数据都在同一时间段内更新，缓存也不会在同一时间内失效，所以每次可坑就是更新少数的缓存。并发量不会特别大。
多服务实例部署的请求路由 如果说你的服务部署了多个实例，那么必须要保证的是，执行数据更新操作，以及执行缓存的更新操作的请求都需要通过Nginx服务器路由到相同的服务实例上
热点商品的路由问题，导致请求的倾斜 比较特殊的情况是，万一某个商品的读写请求都是特别高，全部打到相同的机器和相同的队列里面去了，可能会造成某台机器的负载压力过大。因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发。所以其实要根据实际业务系统查看，如果更新频率不是很高的话这个问题就不是什么大的问题。但还是会有可能造成某些机器的负载压力高一些。
面试官：不错不错。看来实际中确实应用场景比较丰富。
派大星：是的呢。就是简单概括来说： 如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统不是严格要求 “缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：读请求和写请求串行化，串到一个内存队列里去。 串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。
如有问题，欢迎加微信交流：w714771310，或关注微信公众号【码上遇见你】。</description>
    </item>
    
    <item>
      <title></title>
      <link>http://vipbbo.com/1/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B8%80/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://vipbbo.com/1/%E5%AF%B9%E7%BA%BF%E9%9D%A2%E8%AF%95%E5%AE%98-%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%B8%80/</guid>
      <description>面试官：你知道创建线程的方式有哪些吗？派大星：主要有两种方式，一种是通过 Executors 创建，另一种是通过 ThreadPoolExecutorPools 创建。面试官：很好。那你能介绍一下 Executors 提供的 6 种创建方式吗？派大星：当然。Executors 提供了 newFixedThreadPool、newCachedThreadPool、newSingleThreadExecutor、newScheduledThreadPool 等 6 种创建方式。其中 newFixedThreadPool 创建一个固定大小的线程池，可以控制并发的线程数，超出的线程会在队列中等待；newCachedThreadPool 创建一个可以缓存的线程池，若线程数超过处理所需，则会缓存一段时间后回收；newSingleThreadExecutor 创建单个线程数的线程池，它可以保证先进先出的执行顺序；newScheduledThreadPool 创建一个可以执行延迟任务的线程池。面试官：很好。你能说一下这些创建方式的优缺点吗？派大星：当然。但是我要先提醒一下，不建议使用 Executors 创建线程。
FixedThreadPool 和 SingleThreadPool 允许的请求队列长度为 Integer.MAX_VALUE，从而可能会堆积大量请求，造成 OOM； CachedThreadPool 和 ScheduledThreadPool 允许的创建线程数量为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。除此之外，Executors 创建的线程池无法自定义配置，不够灵活。建议使用 ThreadPoolExecutorPools创建方式进行自定义配置 面试官：你能介绍一下 Executors 吗？派大星：Executors 是一个工具类，提供了创建线程池的方法。它提供了 6 种创建线程池的方式，包括 newFixedThreadPool、newCachedThreadPool、newSingleThreadExecutor、newScheduledThreadPool 等。面试官：那你能介绍一下 newFixedThreadPool 的使用方法吗？派大星：当然可以。newFixedThreadPool 可以创建一个固定大小的线程池，可控制并发的线程数，超出的线程会在队列中等待。我们可以通过以下代码来创建一个固定大小的线程池：
ExecutorService threadPool = Executors.newFixedThreadPool(2); 面试官：那你能介绍一下 Executors 提供的其他创建方式吗？派大星：当然可以。除了 newFixedThreadPool，Executors 还提供了其他 5 种创建方式，分别是 newCachedThreadPool、newSingleThreadExecutor、newScheduledThreadPool、newSingleThreadScheduledExecutor 和 newWorkStealingPool。
newCachedThreadPool 可以创建一个可以缓存的线程池，若线程数超过处理所需，则会缓存一段时间后回收。若线程数不够，则新建线程。 newSingleThreadExecutor 可以创建单个线程数的线程池，它可以保证先进先出的执行顺序。 newScheduledThreadPool 可以创建一个可以执行延迟任务的线程池。 newSingleThreadScheduledExecutor 可以创建一个单线程的可以执行延迟任务的线程池。 newWorkStealingPool 可以创建一个抢占式执行的线程池（任务执行顺序不确定），注意此方法只有在 JDK 1.</description>
    </item>
    
  </channel>
</rss>
